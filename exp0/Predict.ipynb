{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datapipeline import Datapipeline, print_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv')\n",
    "labels = df['Churn']\n",
    "df = df.drop('Churn', axis='columns')\n",
    "pl = Datapipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores_acc = []\n",
    "scores_pre = []\n",
    "scores_recall = []\n",
    "scores_f1 = []\n",
    "models = []\n",
    "  \n",
    "for train_index, test_index in skf.split(df, labels):\n",
    "    x_train_fold, x_test_fold = df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = labels.iloc[train_index], labels.iloc[test_index]\n",
    "    model = pl.fit(x_train_fold, y_train_fold)\n",
    "    models.append(model)\n",
    "    pred = pl.predict(x_test_fold)\n",
    "    scores_acc.append(accuracy_score(pred, y_test_fold))\n",
    "    scores_pre.append(precision_score(pred, y_test_fold))\n",
    "    scores_recall.append(recall_score(pred, y_test_fold))\n",
    "    scores_f1.append(f1_score(pred, y_test_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score in fold 0:\n",
      "\tAcc:0.96\tPre:0.89\tRecall:0.87\tF1:0.88\n",
      "Score in fold 1:\n",
      "\tAcc:0.95\tPre:0.90\tRecall:0.84\tF1:0.87\n",
      "Score in fold 2:\n",
      "\tAcc:0.96\tPre:0.88\tRecall:0.88\tF1:0.88\n",
      "Score in fold 3:\n",
      "\tAcc:0.94\tPre:0.84\tRecall:0.79\tF1:0.82\n",
      "Score in fold 4:\n",
      "\tAcc:0.97\tPre:0.92\tRecall:0.89\tF1:0.90\n",
      "Score in fold 5:\n",
      "\tAcc:0.96\tPre:0.89\tRecall:0.90\tF1:0.89\n",
      "Score in fold 6:\n",
      "\tAcc:0.94\tPre:0.84\tRecall:0.83\tF1:0.84\n",
      "Score in fold 7:\n",
      "\tAcc:0.95\tPre:0.82\tRecall:0.86\tF1:0.84\n",
      "Score in fold 8:\n",
      "\tAcc:0.93\tPre:0.85\tRecall:0.78\tF1:0.81\n",
      "Score in fold 9:\n",
      "\tAcc:0.95\tPre:0.87\tRecall:0.84\tF1:0.86\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(\"Score in fold {}:\\n\\tAcc:{:.2f}\\tPre:{:.2f}\\tRecall:{:.2f}\\tF1:{:.2f}\".format(i,scores_acc[i], scores_pre[i], scores_recall[i], scores_f1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score:\n",
      "\tAcc:0.95\tPre:0.87\tRecall:0.85\tF1:0.86\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean score:\\n\\tAcc:{:.2f}\\tPre:{:.2f}\\tRecall:{:.2f}\\tF1:{:.2f}\".format(mean(scores_acc), mean(scores_pre), mean(scores_recall), mean(scores_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../test.csv')\n",
    "labels_test = df_test['Churn']\n",
    "customers_test = df_test.drop('Churn', axis='columns')\n",
    "\n",
    "model_max = models[np.argmax(scores_f1)]\n",
    "pred_test = model_max.predict(customers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9578152753108348\n",
      "Precision 0.8796791443850267\n",
      "Recall 0.8680738786279684\n",
      "F1 0.8738379814077025\n"
     ]
    }
   ],
   "source": [
    "print_score(labels_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a38ed6e5e888fe817b93ac02723f0eef04569b06fa2a91ca7e3d5e390014e359"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
